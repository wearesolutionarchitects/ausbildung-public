{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe: Crawler\n",
    "\n",
    "**Aufgabe:**\n",
    "\n",
    "- Passe den ArticleFetcher so an, dass er die Informationen aus allen Seiten extrahiert\n",
    "\n",
    "Hier nochmal die URL: http://python.beispiel.programmierenlernen.io/index.php\n",
    "\n",
    "**Tipps:**\n",
    "\n",
    "- Schau dir zuerst an, wie du den Button \"Zur nächsten Seite\" ansteuern kannst.\n",
    "- Wie greifst du von Python aus auf das \"href\" - Attribut dieses Buttons zu?\n",
    "- (Optional): Probier ggf. zuerst, nur die Infos der ersten 2 Seiten zu holen. Kannst du darauf aufbauend das Programm verallgemeinern, so dass es alle Seiten einliest?\n",
    "- Du kannst dich daran orientieren, ob es einen \"Zur nächsten Seite\"-Button gibt, oder nicht. Wenn es diesen Button nicht mehr gibt, bist du auf der letzten Seite angelangt. Welcher Schleifentyp bietet sich hier an, wenn du die Schleife erst dann stoppen willst, wenn es den Button nicht mehr gibt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrawledArticle():\n",
    "    def __init__(self, title, emoji, content, image):\n",
    "        self.title = title\n",
    "        self.emoji = emoji\n",
    "        self.content = content\n",
    "        self.image = image\n",
    "        \n",
    "class ArticleFetcher():\n",
    "    def fetch(self):\n",
    "        url = \"http://python.beispiel.programmierenlernen.io/index.php\"\n",
    "        time.sleep(1)\n",
    "        print(url)\n",
    "        r = requests.get(url)\n",
    "        doc = BeautifulSoup(r.text, \"html.parser\")\n",
    "        \n",
    "        articles = []\n",
    "        for card in doc.select(\".card\"):\n",
    "            emoji = card.select_one(\".emoji\").text\n",
    "            content = card.select_one(\".card-text\").text\n",
    "            title = card.select(\".card-title span\")[1].text\n",
    "            image = urljoin(url, card.select_one(\"img\").attrs[\"src\"])\n",
    "\n",
    "            crawled = CrawledArticle(title, emoji, content, image)\n",
    "            articles.append(crawled)\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.CrawledArticle at 0x10dfb36a0>,\n",
       " <__main__.CrawledArticle at 0x10dfb3748>,\n",
       " <__main__.CrawledArticle at 0x10dfb36d8>,\n",
       " <__main__.CrawledArticle at 0x10dfb3780>,\n",
       " <__main__.CrawledArticle at 0x10dfb37b8>,\n",
       " <__main__.CrawledArticle at 0x10dfb37f0>,\n",
       " <__main__.CrawledArticle at 0x10dfb3828>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetcher = ArticleFetcher()\n",
    "fetcher.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
